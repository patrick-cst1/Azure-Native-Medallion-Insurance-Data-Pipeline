{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bronze Layer: Ingest Policies CSV to Delta\n",
        "Azure Synapse Analytics - Medallion Architecture\n",
        "\n",
        "**Pattern**: Read all columns as strings (Medallion best practice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, subprocess\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\", \"-q\"], check=False)\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import current_timestamp, lit, to_date, col, input_file_name\n",
        "import logging\n",
        "import yaml\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - ADLS Gen2 paths (abfss)\n",
        "STORAGE_ACCOUNT = \"<storage-account-name>\"\n",
        "FILES_ROOT = f\"abfss://files@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "TABLES_ROOT = f\"abfss://tables@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "\n",
        "SOURCE_PATH = f\"{FILES_ROOT}/samples/batch/policies.csv\"\n",
        "TARGET_PATH = f\"{TABLES_ROOT}/bronze/bronze_policies\"\n",
        "PARTITION_COLUMN = \"ingestion_date\"\n",
        "SCHEMA_PATH = f\"{FILES_ROOT}/config/schemas/bronze/bronze_policies.yaml\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_schema(df, schema_path):\n",
        "    \"\"\"Simplified inline schema validation.\"\"\"\n",
        "    try:\n",
        "        schema_content = spark.read.text(schema_path, wholetext=True).collect()[0][0]\n",
        "        schema = yaml.safe_load(schema_content)\n",
        "        \n",
        "        for col_def in schema['required_columns']:\n",
        "            col_name = col_def['name']\n",
        "            nullable = col_def['nullable']\n",
        "            \n",
        "            if col_name not in df.columns:\n",
        "                raise ValueError(f\"Missing required column: {col_name}\")\n",
        "            \n",
        "            if not nullable:\n",
        "                null_count = df.filter(col(col_name).isNull()).count()\n",
        "                if null_count > 0:\n",
        "                    logger.warning(f\"Found {null_count} null values in non-nullable column: {col_name}\")\n",
        "        \n",
        "        logger.info(\"✓ Schema validation passed\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Schema validation skipped: {str(e)}\")\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Ingest policies from CSV to Bronze Delta table.\"\"\"\n",
        "    \n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Reading policies from {SOURCE_PATH}\")\n",
        "        df = spark.read.format(\"csv\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"false\") \\\n",
        "            .load(SOURCE_PATH)\n",
        "        \n",
        "        process_id = str(uuid.uuid4())\n",
        "        \n",
        "        df_enriched = df \\\n",
        "            .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
        "            .withColumn(\"ingestion_date\", to_date(current_timestamp())) \\\n",
        "            .withColumn(\"source_system\", lit(\"legacy_csv\")) \\\n",
        "            .withColumn(\"process_id\", lit(process_id)) \\\n",
        "            .withColumn(\"source_file_name\", input_file_name())\n",
        "        \n",
        "        record_count = df_enriched.count()\n",
        "        logger.info(f\"Read {record_count} policies\")\n",
        "        \n",
        "        validate_schema(df_enriched, SCHEMA_PATH)\n",
        "        \n",
        "        logger.info(f\"Writing to {TARGET_PATH}\")\n",
        "        df_enriched.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .mode(\"append\") \\\n",
        "            .partitionBy(PARTITION_COLUMN) \\\n",
        "            .option(\"mergeSchema\", \"true\") \\\n",
        "            .save(TARGET_PATH)\n",
        "        \n",
        "        logger.info(\"✓ Bronze policies ingestion completed\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"✗ Policies ingestion failed: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
