{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Silver Layer: Clean and Standardize Claims\n",
        "Azure Synapse Analytics - Medallion Architecture\n",
        "\n",
        "**Pattern**: Schema-driven type casting + validation + SCD Type 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, trim, upper, current_timestamp, lit, to_date\n",
        "import logging\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - ADLS Gen2 paths\n",
        "STORAGE_ACCOUNT = \"<storage-account-name>\"\n",
        "TABLES_ROOT = f\"abfss://tables@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "FILES_ROOT = f\"abfss://files@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "\n",
        "BRONZE_PATH = f\"{TABLES_ROOT}/bronze/bronze_claims\"\n",
        "SILVER_PATH = f\"{TABLES_ROOT}/silver/silver_claims\"\n",
        "SCHEMA_PATH = f\"{FILES_ROOT}/config/schemas/silver/silver_claims.yaml\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_schema_transformations(df, schema_path):\n",
        "    \"\"\"Apply type casting and transformations based on silver schema.\"\"\"\n",
        "    try:\n",
        "        schema_content = spark.read.text(schema_path, wholetext=True).collect()[0][0]\n",
        "        schema = yaml.safe_load(schema_content)\n",
        "    except:\n",
        "        logger.warning(f\"Schema not found, skipping transformations\")\n",
        "        return df\n",
        "    \n",
        "    for col_def in schema['business_columns']:\n",
        "        col_name = col_def['name']\n",
        "        col_type = col_def['type']\n",
        "        transformation = col_def.get('transformation', None)\n",
        "        \n",
        "        # Apply transformation first\n",
        "        if transformation == 'upper_trim':\n",
        "            df = df.withColumn(col_name, upper(trim(col(col_name))))\n",
        "        elif transformation == 'trim':\n",
        "            df = df.withColumn(col_name, trim(col(col_name)))\n",
        "        \n",
        "        # Apply type casting\n",
        "        if col_type == 'double':\n",
        "            df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
        "        elif col_type == 'integer':\n",
        "            df = df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
        "        elif col_type == 'date':\n",
        "            df = df.withColumn(col_name, to_date(col(col_name)))\n",
        "        \n",
        "        # Apply nullable filter\n",
        "        if not col_def['nullable']:\n",
        "            df = df.filter(col(col_name).isNotNull())\n",
        "        \n",
        "        # Apply validation rules\n",
        "        if 'validation' in col_def:\n",
        "            for rule in col_def['validation']:\n",
        "                if rule['rule'] == 'greater_than':\n",
        "                    df = df.filter(col(col_name) > rule['value'])\n",
        "                elif rule['rule'] == 'less_than':\n",
        "                    df = df.filter(col(col_name) < rule['value'])\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Reading from {BRONZE_PATH}\")\n",
        "        df_bronze = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
        "        \n",
        "        record_count = df_bronze.count()\n",
        "        logger.info(f\"Read {record_count} records from Bronze\")\n",
        "        \n",
        "        # Deduplication\n",
        "        df_cleaned = df_bronze.dropDuplicates([\"claim_id\"])\n",
        "        \n",
        "        # Apply schema-driven transformations\n",
        "        logger.info(f\"Applying silver schema transformations from {SCHEMA_PATH}\")\n",
        "        df_cleaned = apply_schema_transformations(df_cleaned, SCHEMA_PATH)\n",
        "        \n",
        "        # Add processing timestamp\n",
        "        df_cleaned = df_cleaned.withColumn(\"processed_timestamp\", current_timestamp())\n",
        "        \n",
        "        # Add SCD Type 2 columns\n",
        "        df_cleaned = df_cleaned \\\n",
        "            .withColumn(\"effective_from\", col(\"ingestion_timestamp\")) \\\n",
        "            .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
        "            .withColumn(\"is_current\", lit(True))\n",
        "        \n",
        "        cleaned_count = df_cleaned.count()\n",
        "        dropped_count = record_count - cleaned_count\n",
        "        pass_rate = (cleaned_count / record_count * 100) if record_count > 0 else 0\n",
        "        \n",
        "        logger.info(f\"Data Quality Metrics:\")\n",
        "        logger.info(f\"  - Total records: {record_count}\")\n",
        "        logger.info(f\"  - Cleaned records: {cleaned_count}\")\n",
        "        logger.info(f\"  - Dropped records: {dropped_count}\")\n",
        "        logger.info(f\"  - Pass rate: {pass_rate:.2f}%\")\n",
        "        \n",
        "        # Write to Silver\n",
        "        logger.info(f\"Writing to {SILVER_PATH}\")\n",
        "        df_cleaned.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .option(\"description\", \"Silver layer: Cleaned claims with SCD Type 2 tracking\") \\\n",
        "            .save(SILVER_PATH)\n",
        "        \n",
        "        logger.info(\"✓ Silver claims cleaning completed\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"✗ Failed to clean claims: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
